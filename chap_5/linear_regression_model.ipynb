{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Einfache lineare Regression mit TensorFlow 2.x\n",
    "#\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) # Damit wir immer die gleichen Zufallswerte bekommen\n",
    "my_weight = 4 # Diese Variable muss später vom Modell gelernt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ein Array von 100 Werten wird generiert.\n",
    "input = np.arange(0, 10, 0.1,dtype=\"float32\")\n",
    "noise = np.random.uniform(-1,1,size=input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Damit die Ausgabe nicht direkt linear ist, werden Zufallswerte hinzugefügt.\n",
    "output = my_weight * input + noise\n",
    "output = output.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "plt.title('Funktion y=x*w + b')\n",
    "plt.scatter(input, output, c=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MyLinearRegressionModel():\n",
    "    def __init__(self):\n",
    "        self.W = tf.Variable(np.random.uniform(),dtype=\"float32\",trainable=True )\n",
    "        self.b = tf.Variable(np.random.uniform(),dtype=\"float32\",trainable=True )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return tf.add(tf.multiply(x,self.W), self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "model = MyLinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def loss_function(pred, y):\n",
    "    return tf.reduce_mean(tf.square(pred - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "learning_rate=0.01\n",
    "optimizer = tf.keras.optimizers.SGD(lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    " # 1. Version\n",
    "def train(model,x,y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        current_loss = loss_function(model(input), output)  \n",
    "    \n",
    "    dW, db  = tape.gradient(current_loss, [model.W,model.b])\n",
    "    model.W.assign_sub(learning_rate * dW)\n",
    "    model.b.assign_sub(learning_rate * db )\n",
    "    return current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 2. Version mit tape.gradient() und optimizer\n",
    "def train_with_optimizers(model,x,y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        current_loss = loss_function(model(input), output)  \n",
    "    gradients = tape.gradient(current_loss,[model.W,model.b])\n",
    "    optimizer.apply_gradients(zip(gradients , [model.W,model.b]))\n",
    "    return current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "plt.ion()\n",
    "plt.title('Funktion')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    " # Update des plt.scatter() mit den neu berechneteten Werten\n",
    "def redraw_curve(input,output,predicted_output,epoch,loss):\n",
    "    plt.clf()\n",
    "    plt.title(\"Epoch: \" +str(epoch) +'\\nLoss: ' + str(loss.numpy())+'\\nFunktion y = x*w + b  mit w=' + str(model.W.numpy()) + ' und b=' + str(model.b.numpy()))\n",
    "    plt.scatter(input,output,c=\"red\",s=4,label=\"Original Werte\")\n",
    "    plt.scatter(input,predicted_output,s=5, c=\"g\", label=\"Vorhersage\")\n",
    "    plt.show()  \n",
    "    plt.draw()\n",
    "    plt.pause(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainingsschleife\n",
    "for epoch in range(0,500):\n",
    "    # Version 1:\n",
    "    loss = train_with_optimizers(model, input,output)\n",
    "    # Version 2:\n",
    "    # loss = train(model,input,output)\n",
    "    print(\"Current loss: {}\".format(loss.numpy()))\n",
    "    predicted_output = model(input) \n",
    "    redraw_curve(input,output,predicted_output,epoch,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show(block=True) \n",
    "print(\"Vorhersage Wert für w: \" + str(model.W.numpy()))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
