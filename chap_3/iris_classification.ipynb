{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Beispiel der Klassifikations von Iris-Blumen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lade den Iris-Datenset\n",
    "data_train = pd.read_csv('./iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die 3 zu erkennenden Klassifikationsklassen werden zu numerischen Werten 0, 1 bzw. 2 umgewandelt.\n",
    "data_train.loc[data_train['species']=='Iris-setosa', 'species']=0\n",
    "data_train.loc[data_train['species']=='Iris-versicolor', 'species']=1\n",
    "data_train.loc[data_train['species']=='Iris-virginica', 'species']=2\n",
    "data_train = data_train.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Der eingelesene Datenset wird als Matrix dargestellt\n",
    "data_train_array = data_train.values # oder data_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zur Sicherstellung der Reproduzierbarkeit der Ergebnisse setzen wir random.seed auf eine festen Wert, z.B. 42\n",
    "np.random.seed(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Datenset wird in zwei separate Kategorie gespaltet: Testdaten und Trainingsdaten. \n",
    "80% der Daten werden zum Trainieren und 20% zum Testen des Modells verwendet. \n",
    "Da es sich bei der Eingabe um einen Vektor handelt, werden wird den Großbuchstaben X benutzen; \n",
    "Für die Ausgabe hingegen handelt es sich um ein einzelner Werte, \n",
    "daher die Bezeichung mit dem Kleinbuchstaben y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_train_array[:,:4],\n",
    "                                                    data_train_array[:,4],\n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION 1\n",
    "# Ein neuronales Netz zur Klassifikation (MultiLayerPerceptron) wird mit folgenden Eigenschaften gebildet:\n",
    "# einem Input-Layer mit 4 Neuronen, die die Merkmale der Iris-Planze repräsentieren;\n",
    "# einem Hidden-Layer mit 10 Neuronen\n",
    "# eime Output-Layer mit 4 Neuronen, die die zu erkennenden Klassen repräsentieren.\n",
    "# Dabei wird als Aktivierungsfunktion relu und als Optimierer adam verwenden.\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,),activation='relu', solver='adam', max_iter=350, batch_size=10, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VERSION 2\n",
    "In der zweiten Variante werden 2 Hidden-Layers mit jeweils 5 bzw. 3 Neuronen verwendet\n",
    "Dabei wird als Aktivierungsfunktion tanh und als Optimierer adam verwenden. \n",
    "mlp = MLPClassifier(hidden_layer_sizes=(5,3),activation='tanh', solver='adam', max_iter=350, batch_size=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.00250272\n",
      "Iteration 2, loss = 3.40553964\n",
      "Iteration 3, loss = 2.83501057\n",
      "Iteration 4, loss = 2.31506032\n",
      "Iteration 5, loss = 1.83550853\n",
      "Iteration 6, loss = 1.51090872\n",
      "Iteration 7, loss = 1.32850876\n",
      "Iteration 8, loss = 1.24046965\n",
      "Iteration 9, loss = 1.17041371\n",
      "Iteration 10, loss = 1.12110589\n",
      "Iteration 11, loss = 1.07358008\n",
      "Iteration 12, loss = 1.03679092\n",
      "Iteration 13, loss = 1.00474070\n",
      "Iteration 14, loss = 0.97298690\n",
      "Iteration 15, loss = 0.94589371\n",
      "Iteration 16, loss = 0.92051278\n",
      "Iteration 17, loss = 0.89480893\n",
      "Iteration 18, loss = 0.87292932\n",
      "Iteration 19, loss = 0.84832888\n",
      "Iteration 20, loss = 0.82650054\n",
      "Iteration 21, loss = 0.80645614\n",
      "Iteration 22, loss = 0.78835510\n",
      "Iteration 23, loss = 0.76874525\n",
      "Iteration 24, loss = 0.75254667\n",
      "Iteration 25, loss = 0.73259700\n",
      "Iteration 26, loss = 0.71952956\n",
      "Iteration 27, loss = 0.70244477\n",
      "Iteration 28, loss = 0.68857793\n",
      "Iteration 29, loss = 0.67245257\n",
      "Iteration 30, loss = 0.66097361\n",
      "Iteration 31, loss = 0.64636006\n",
      "Iteration 32, loss = 0.63455522\n",
      "Iteration 33, loss = 0.62332761\n",
      "Iteration 34, loss = 0.61314445\n",
      "Iteration 35, loss = 0.60200208\n",
      "Iteration 36, loss = 0.59365949\n",
      "Iteration 37, loss = 0.58603645\n",
      "Iteration 38, loss = 0.57395705\n",
      "Iteration 39, loss = 0.56471821\n",
      "Iteration 40, loss = 0.55661838\n",
      "Iteration 41, loss = 0.54943590\n",
      "Iteration 42, loss = 0.54118233\n",
      "Iteration 43, loss = 0.53311911\n",
      "Iteration 44, loss = 0.52634641\n",
      "Iteration 45, loss = 0.52357651\n",
      "Iteration 46, loss = 0.51608916\n",
      "Iteration 47, loss = 0.50697203\n",
      "Iteration 48, loss = 0.50110485\n",
      "Iteration 49, loss = 0.49573385\n",
      "Iteration 50, loss = 0.48958652\n",
      "Iteration 51, loss = 0.48114664\n",
      "Iteration 52, loss = 0.47741165\n",
      "Iteration 53, loss = 0.46510576\n",
      "Iteration 54, loss = 0.46048672\n",
      "Iteration 55, loss = 0.45040712\n",
      "Iteration 56, loss = 0.44371987\n",
      "Iteration 57, loss = 0.43589618\n",
      "Iteration 58, loss = 0.43091133\n",
      "Iteration 59, loss = 0.42159497\n",
      "Iteration 60, loss = 0.41455269\n",
      "Iteration 61, loss = 0.40858428\n",
      "Iteration 62, loss = 0.40152708\n",
      "Iteration 63, loss = 0.39565112\n",
      "Iteration 64, loss = 0.38891406\n",
      "Iteration 65, loss = 0.38314380\n",
      "Iteration 66, loss = 0.37798783\n",
      "Iteration 67, loss = 0.37205915\n",
      "Iteration 68, loss = 0.36731584\n",
      "Iteration 69, loss = 0.36172866\n",
      "Iteration 70, loss = 0.35600670\n",
      "Iteration 71, loss = 0.35055225\n",
      "Iteration 72, loss = 0.34615675\n",
      "Iteration 73, loss = 0.34143502\n",
      "Iteration 74, loss = 0.33589246\n",
      "Iteration 75, loss = 0.33253429\n",
      "Iteration 76, loss = 0.32701748\n",
      "Iteration 77, loss = 0.32195726\n",
      "Iteration 78, loss = 0.31894041\n",
      "Iteration 79, loss = 0.31385268\n",
      "Iteration 80, loss = 0.31002660\n",
      "Iteration 81, loss = 0.30643779\n",
      "Iteration 82, loss = 0.30218609\n",
      "Iteration 83, loss = 0.29715981\n",
      "Iteration 84, loss = 0.29543140\n",
      "Iteration 85, loss = 0.29061314\n",
      "Iteration 86, loss = 0.28954316\n",
      "Iteration 87, loss = 0.28586530\n",
      "Iteration 88, loss = 0.27882506\n",
      "Iteration 89, loss = 0.27790751\n",
      "Iteration 90, loss = 0.27268603\n",
      "Iteration 91, loss = 0.27053940\n",
      "Iteration 92, loss = 0.26629795\n",
      "Iteration 93, loss = 0.26412826\n",
      "Iteration 94, loss = 0.26324762\n",
      "Iteration 95, loss = 0.25607121\n",
      "Iteration 96, loss = 0.25705462\n",
      "Iteration 97, loss = 0.25020809\n",
      "Iteration 98, loss = 0.25001839\n",
      "Iteration 99, loss = 0.24403819\n",
      "Iteration 100, loss = 0.24379350\n",
      "Iteration 101, loss = 0.24305067\n",
      "Iteration 102, loss = 0.23614715\n",
      "Iteration 103, loss = 0.23382451\n",
      "Iteration 104, loss = 0.23075420\n",
      "Iteration 105, loss = 0.22975345\n",
      "Iteration 106, loss = 0.22583900\n",
      "Iteration 107, loss = 0.22694159\n",
      "Iteration 108, loss = 0.22680338\n",
      "Iteration 109, loss = 0.21763400\n",
      "Iteration 110, loss = 0.21740362\n",
      "Iteration 111, loss = 0.21586797\n",
      "Iteration 112, loss = 0.21190035\n",
      "Iteration 113, loss = 0.21014268\n",
      "Iteration 114, loss = 0.20762820\n",
      "Iteration 115, loss = 0.20484095\n",
      "Iteration 116, loss = 0.20269286\n",
      "Iteration 117, loss = 0.20123277\n",
      "Iteration 118, loss = 0.19833841\n",
      "Iteration 119, loss = 0.19644333\n",
      "Iteration 120, loss = 0.19567437\n",
      "Iteration 121, loss = 0.19342245\n",
      "Iteration 122, loss = 0.19173993\n",
      "Iteration 123, loss = 0.19072406\n",
      "Iteration 124, loss = 0.19032356\n",
      "Iteration 125, loss = 0.18723596\n",
      "Iteration 126, loss = 0.18458529\n",
      "Iteration 127, loss = 0.18270626\n",
      "Iteration 128, loss = 0.18171213\n",
      "Iteration 129, loss = 0.17858243\n",
      "Iteration 130, loss = 0.17747241\n",
      "Iteration 131, loss = 0.17598339\n",
      "Iteration 132, loss = 0.17367914\n",
      "Iteration 133, loss = 0.17382476\n",
      "Iteration 134, loss = 0.17009089\n",
      "Iteration 135, loss = 0.17160257\n",
      "Iteration 136, loss = 0.17062764\n",
      "Iteration 137, loss = 0.16534896\n",
      "Iteration 138, loss = 0.16446967\n",
      "Iteration 139, loss = 0.16582767\n",
      "Iteration 140, loss = 0.16111476\n",
      "Iteration 141, loss = 0.16061014\n",
      "Iteration 142, loss = 0.15994230\n",
      "Iteration 143, loss = 0.15759381\n",
      "Iteration 144, loss = 0.15594770\n",
      "Iteration 145, loss = 0.15606202\n",
      "Iteration 146, loss = 0.15450524\n",
      "Iteration 147, loss = 0.15317786\n",
      "Iteration 148, loss = 0.15146931\n",
      "Iteration 149, loss = 0.15023079\n",
      "Iteration 150, loss = 0.14920274\n",
      "Iteration 151, loss = 0.15225785\n",
      "Iteration 152, loss = 0.14625697\n",
      "Iteration 153, loss = 0.14547466\n",
      "Iteration 154, loss = 0.14421974\n",
      "Iteration 155, loss = 0.14351199\n",
      "Iteration 156, loss = 0.14291335\n",
      "Iteration 157, loss = 0.14455087\n",
      "Iteration 158, loss = 0.14157281\n",
      "Iteration 159, loss = 0.14013531\n",
      "Iteration 160, loss = 0.13834233\n",
      "Iteration 161, loss = 0.13918290\n",
      "Iteration 162, loss = 0.13758428\n",
      "Iteration 163, loss = 0.13471172\n",
      "Iteration 164, loss = 0.13488043\n",
      "Iteration 165, loss = 0.13286354\n",
      "Iteration 166, loss = 0.13289850\n",
      "Iteration 167, loss = 0.13139205\n",
      "Iteration 168, loss = 0.13143697\n",
      "Iteration 169, loss = 0.12990592\n",
      "Iteration 170, loss = 0.12916179\n",
      "Iteration 171, loss = 0.12828003\n",
      "Iteration 172, loss = 0.12725125\n",
      "Iteration 173, loss = 0.12685802\n",
      "Iteration 174, loss = 0.12657482\n",
      "Iteration 175, loss = 0.12432796\n",
      "Iteration 176, loss = 0.12389855\n",
      "Iteration 177, loss = 0.12395765\n",
      "Iteration 178, loss = 0.12293427\n",
      "Iteration 179, loss = 0.12190641\n",
      "Iteration 180, loss = 0.12044426\n",
      "Iteration 181, loss = 0.12085807\n",
      "Iteration 182, loss = 0.11999239\n",
      "Iteration 183, loss = 0.12106574\n",
      "Iteration 184, loss = 0.11784849\n",
      "Iteration 185, loss = 0.11731183\n",
      "Iteration 186, loss = 0.11679709\n",
      "Iteration 187, loss = 0.11582289\n",
      "Iteration 188, loss = 0.11505195\n",
      "Iteration 189, loss = 0.11624004\n",
      "Iteration 190, loss = 0.11365127\n",
      "Iteration 191, loss = 0.11385216\n",
      "Iteration 192, loss = 0.11362652\n",
      "Iteration 193, loss = 0.11320538\n",
      "Iteration 194, loss = 0.11127776\n",
      "Iteration 195, loss = 0.11056493\n",
      "Iteration 196, loss = 0.11019247\n",
      "Iteration 197, loss = 0.10938434\n",
      "Iteration 198, loss = 0.10907792\n",
      "Iteration 199, loss = 0.11094108\n",
      "Iteration 200, loss = 0.10911017\n",
      "Iteration 201, loss = 0.10688460\n",
      "Iteration 202, loss = 0.10677177\n",
      "Iteration 203, loss = 0.10621839\n",
      "Iteration 204, loss = 0.10550440\n",
      "Iteration 205, loss = 0.10540597\n",
      "Iteration 206, loss = 0.10433860\n",
      "Iteration 207, loss = 0.10416424\n",
      "Iteration 208, loss = 0.10373615\n",
      "Iteration 209, loss = 0.10304358\n",
      "Iteration 210, loss = 0.10238204\n",
      "Iteration 211, loss = 0.10178099\n",
      "Iteration 212, loss = 0.10253745\n",
      "Iteration 213, loss = 0.10065243\n",
      "Iteration 214, loss = 0.10139100\n",
      "Iteration 215, loss = 0.10473503\n",
      "Iteration 216, loss = 0.10236394\n",
      "Iteration 217, loss = 0.09989665\n",
      "Iteration 218, loss = 0.09891096\n",
      "Iteration 219, loss = 0.09939591\n",
      "Iteration 220, loss = 0.10119130\n",
      "Iteration 221, loss = 0.09706190\n",
      "Iteration 222, loss = 0.09798006\n",
      "Iteration 223, loss = 0.09707054\n",
      "Iteration 224, loss = 0.09713712\n",
      "Iteration 225, loss = 0.09660875\n",
      "Iteration 226, loss = 0.09663119\n",
      "Iteration 227, loss = 0.09484372\n",
      "Iteration 228, loss = 0.09656171\n",
      "Iteration 229, loss = 0.09458237\n",
      "Iteration 230, loss = 0.09427047\n",
      "Iteration 231, loss = 0.09362427\n",
      "Iteration 232, loss = 0.09308312\n",
      "Iteration 233, loss = 0.09325394\n",
      "Iteration 234, loss = 0.09228941\n",
      "Iteration 235, loss = 0.09241620\n",
      "Iteration 236, loss = 0.09151906\n",
      "Iteration 237, loss = 0.09266828\n",
      "Iteration 238, loss = 0.08962935\n",
      "Iteration 239, loss = 0.09229031\n",
      "Iteration 240, loss = 0.08964163\n",
      "Iteration 241, loss = 0.09028065\n",
      "Iteration 242, loss = 0.08936245\n",
      "Iteration 243, loss = 0.08932396\n",
      "Iteration 244, loss = 0.08876262\n",
      "Iteration 245, loss = 0.08839279\n",
      "Iteration 246, loss = 0.08778860\n",
      "Iteration 247, loss = 0.09186589\n",
      "Iteration 248, loss = 0.08601158\n",
      "Iteration 249, loss = 0.08913588\n",
      "Iteration 250, loss = 0.08830487\n",
      "Iteration 251, loss = 0.08715911\n",
      "Iteration 252, loss = 0.08720084\n",
      "Iteration 253, loss = 0.08595909\n",
      "Iteration 254, loss = 0.08659991\n",
      "Iteration 255, loss = 0.08606938\n",
      "Iteration 256, loss = 0.08532553\n",
      "Iteration 257, loss = 0.08559611\n",
      "Iteration 258, loss = 0.08577490\n",
      "Iteration 259, loss = 0.08557039\n",
      "Iteration 260, loss = 0.08507476\n",
      "Iteration 261, loss = 0.08459227\n",
      "Iteration 262, loss = 0.08612319\n",
      "Iteration 263, loss = 0.08522679\n",
      "Iteration 264, loss = 0.08263237\n",
      "Iteration 265, loss = 0.08287833\n",
      "Iteration 266, loss = 0.08212850\n",
      "Iteration 267, loss = 0.08355807\n",
      "Iteration 268, loss = 0.08439522\n",
      "Iteration 269, loss = 0.08366551\n",
      "Iteration 270, loss = 0.08133942\n",
      "Iteration 271, loss = 0.08152976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 272, loss = 0.08097790\n",
      "Iteration 273, loss = 0.08133977\n",
      "Iteration 274, loss = 0.08143214\n",
      "Iteration 275, loss = 0.08004292\n",
      "Iteration 276, loss = 0.08064937\n",
      "Iteration 277, loss = 0.08015252\n",
      "Iteration 278, loss = 0.07909858\n",
      "Iteration 279, loss = 0.07905402\n",
      "Iteration 280, loss = 0.07860276\n",
      "Iteration 281, loss = 0.07951599\n",
      "Iteration 282, loss = 0.07861664\n",
      "Iteration 283, loss = 0.07909591\n",
      "Iteration 284, loss = 0.07782827\n",
      "Iteration 285, loss = 0.07948777\n",
      "Iteration 286, loss = 0.07705228\n",
      "Iteration 287, loss = 0.07784224\n",
      "Iteration 288, loss = 0.07723424\n",
      "Iteration 289, loss = 0.07692231\n",
      "Iteration 290, loss = 0.07713531\n",
      "Iteration 291, loss = 0.07943792\n",
      "Iteration 292, loss = 0.07640899\n",
      "Iteration 293, loss = 0.07587269\n",
      "Iteration 294, loss = 0.07601615\n",
      "Iteration 295, loss = 0.07640203\n",
      "Iteration 296, loss = 0.07544157\n",
      "Iteration 297, loss = 0.07639115\n",
      "Iteration 298, loss = 0.07644806\n",
      "Iteration 299, loss = 0.07570033\n",
      "Iteration 300, loss = 0.07449521\n",
      "Iteration 301, loss = 0.07520592\n",
      "Iteration 302, loss = 0.07855861\n",
      "Iteration 303, loss = 0.07307318\n",
      "Iteration 304, loss = 0.07550570\n",
      "Iteration 305, loss = 0.07365299\n",
      "Iteration 306, loss = 0.07426593\n",
      "Iteration 307, loss = 0.07338815\n",
      "Iteration 308, loss = 0.07582690\n",
      "Iteration 309, loss = 0.07442961\n",
      "Iteration 310, loss = 0.07351230\n",
      "Iteration 311, loss = 0.07509967\n",
      "Iteration 312, loss = 0.07195877\n",
      "Iteration 313, loss = 0.07312217\n",
      "Iteration 314, loss = 0.07253805\n",
      "Iteration 315, loss = 0.07231421\n",
      "Iteration 316, loss = 0.07167338\n",
      "Iteration 317, loss = 0.07313583\n",
      "Iteration 318, loss = 0.07725933\n",
      "Iteration 319, loss = 0.07099868\n",
      "Iteration 320, loss = 0.07177973\n",
      "Iteration 321, loss = 0.07218251\n",
      "Iteration 322, loss = 0.07257143\n",
      "Iteration 323, loss = 0.07144441\n",
      "Iteration 324, loss = 0.07107552\n",
      "Iteration 325, loss = 0.07030424\n",
      "Iteration 326, loss = 0.07067954\n",
      "Iteration 327, loss = 0.07030101\n",
      "Iteration 328, loss = 0.07029363\n",
      "Iteration 329, loss = 0.06954314\n",
      "Iteration 330, loss = 0.07194073\n",
      "Iteration 331, loss = 0.07057397\n",
      "Iteration 332, loss = 0.07004004\n",
      "Iteration 333, loss = 0.07165578\n",
      "Iteration 334, loss = 0.06940251\n",
      "Iteration 335, loss = 0.07100749\n",
      "Iteration 336, loss = 0.06918533\n",
      "Iteration 337, loss = 0.06916999\n",
      "Iteration 338, loss = 0.07195196\n",
      "Iteration 339, loss = 0.06954407\n",
      "Iteration 340, loss = 0.06860971\n",
      "Iteration 341, loss = 0.06931413\n",
      "Iteration 342, loss = 0.07000790\n",
      "Iteration 343, loss = 0.06912624\n",
      "Iteration 344, loss = 0.06787945\n",
      "Iteration 345, loss = 0.06835903\n",
      "Iteration 346, loss = 0.06762316\n",
      "Iteration 347, loss = 0.07096147\n",
      "Iteration 348, loss = 0.06972997\n",
      "Iteration 349, loss = 0.07082944\n",
      "Iteration 350, loss = 0.06655910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wootpecker\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(batch_size=10, hidden_layer_sizes=(10,), max_iter=350,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Das neuronale Netz wird mit den Trainingsdaten traniert\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsergebnis: 0.983\n"
     ]
    }
   ],
   "source": [
    "# Das Ergebnis des Training wird ausgegeben\n",
    "print(\"Trainingsergebnis: %5.3f\" % mlp.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  0  0]\n",
      " [ 0 11  0]\n",
      " [ 0  1 11]]\n"
     ]
    }
   ],
   "source": [
    "# Das Modell wird mit den Testdatensdaten evaluiert\n",
    "predictions = mlp.predict(X_test)\n",
    "# und die Konfusionsmatrix ausgegeben\n",
    "print(confusion_matrix(y_test,predictions))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         7\n",
      "         1.0       0.92      1.00      0.96        11\n",
      "         2.0       1.00      0.92      0.96        12\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aus der Konfusionsmatrix werden precison, recall und f1-score berechnet und ausgebenen\n",
    "print(classification_report(y_test,predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testergebnis: 0.967\n"
     ]
    }
   ],
   "source": [
    "# Das Modell wird getest und das Ergebnis ausgegeben\n",
    "print(\"Testergebnis: %5.3f\" % mlp.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHTS: [array([[-2.82928616e-02,  3.98626014e-01, -5.97650566e-02,\n",
      "         3.60177468e-01,  8.35854375e-02,  4.61691767e-67,\n",
      "         5.58140154e-01,  3.03496513e-01,  3.98363846e-01,\n",
      "        -2.26749801e-02],\n",
      "       [ 6.69030070e-01, -3.14379912e-01, -9.14913685e-02,\n",
      "         5.20526479e-01, -6.07593114e-01, -3.72425974e-98,\n",
      "         3.55825014e-01,  6.43137144e-01,  4.05330555e-01,\n",
      "        -3.71695990e-01],\n",
      "       [ 5.68242922e-01,  6.41685919e-01, -5.92638144e-02,\n",
      "        -5.45713391e-01,  1.42606035e+00, -8.59704250e-22,\n",
      "        -2.29152900e-03,  5.22687734e-01,  1.62946479e-02,\n",
      "         4.20088178e-02],\n",
      "       [ 3.74815797e-01,  5.69459224e-01, -2.24764024e-02,\n",
      "        -6.67575691e-01,  1.20578245e+00, -1.95510383e-20,\n",
      "        -9.93512796e-01, -1.93401619e-01,  2.19345212e-01,\n",
      "         1.08464432e+00]]), array([[ 4.95433091e-01, -4.86281052e-01,  4.44882544e-01],\n",
      "       [-7.49077340e-02, -5.77231816e-02,  3.09369926e-01],\n",
      "       [ 5.35374686e-02, -8.18924816e-03, -5.34071722e-02],\n",
      "       [ 2.16582651e+00,  1.09421618e+00, -2.25413978e+00],\n",
      "       [-2.04139219e+00,  6.33446836e-02,  7.50051333e-01],\n",
      "       [-1.35138137e-22, -8.52400906e-30,  7.23022956e-94],\n",
      "       [ 7.67483959e-01,  3.42160066e-01, -6.39654527e-01],\n",
      "       [-2.53905584e-01,  1.40318999e-01, -7.03359532e-01],\n",
      "       [ 1.04147974e-01,  1.19211175e-01,  2.71911450e-01],\n",
      "       [-3.39491711e-01, -9.52355438e-01,  9.09437781e-01]])]\n",
      "BIASES: [array([-0.46699862,  0.1718756 ,  0.4377293 ,  1.1591752 , -0.80572818,\n",
      "       -0.19448985, -0.02452142,  0.66509048, -0.52902837, -0.31067644]), array([-0.05163483, -0.25541857, -1.0410276 ])]\n"
     ]
    }
   ],
   "source": [
    "# Folgendes gibt die Werte der Gewichte pro Layer aus\n",
    "print(\"WEIGHTS:\", mlp.coefs_)\n",
    "print(\"BIASES:\", mlp.intercepts_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 2. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Das Modell wird beispielsweise zur Vorhersage auf folgenden Werten \n",
    "# aus dem Testset angewandt mit den Merkmalen [sepal-length, sepal-width, \n",
    "# petal-length, petal-width]\n",
    "print(mlp.predict([[5.1,3.5,1.4,0.2], [5.9,3.,5.1,1.8], [4.9,3.,1.4,0.2], [5.8,2.7,4.1,1.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Die Loss-Kurve wird visualisiert und in der Datei Plot_of_loss_values.png im PNG-Format gespeichert.\n",
    "loss_values = mlp.loss_curve_\n",
    "plt.plot(loss_values)\n",
    "plt.savefig(\"./Plot_of_loss_values.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
